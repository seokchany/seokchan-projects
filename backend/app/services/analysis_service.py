# app/services/analysis_service.py
import uuid
import json
import asyncio
import time
import hashlib
import logging
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

import pandas as pd
from pydantic import ValidationError
from sqlalchemy.ext.asyncio import AsyncSession
from elasticsearch.helpers import async_bulk

# --- ì• í”Œë¦¬ì¼€ì´ì…˜ ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from app.core.redis_client import redis_client
from app.core.preprocessing import map_sysmon_to_model_columns, fill_and_mask_missing_features
from app.core.database import AsyncSessionLocal, es_client
from src.core.config import settings
from app.ml.predictor import predictor
from app.models.models import AttackLog, AttackTraffic
from app.schemas.schemas import RawTrafficData

# --- ë¡œê±°(Logger) ì„¤ì • ---
# ì„œë¹„ìŠ¤ ì „ë°˜ì˜ ì´ë²¤íŠ¸ ê¸°ë¡ì„ ìœ„í•´ í‘œì¤€ ë¡œê¹… ëª¨ë“ˆì„ ì„¤ì •í•©ë‹ˆë‹¤.
# ë¡œê·¸ ë ˆë²¨ì€ INFOë¡œ ì„¤ì •í•˜ì—¬ ì •ë³´ì„±, ê²½ê³ , ì˜¤ë¥˜ ë¡œê·¸ë¥¼ ëª¨ë‘ ì¶œë ¥í•©ë‹ˆë‹¤.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- ìƒìˆ˜(Constants) ì •ì˜ ---
# Winlogbeat ë¡œê·¸ì—ì„œ IP ì£¼ì†Œë¥¼ ì°¾ê¸° ìœ„í•œ í›„ë³´ í•„ë“œ ê²½ë¡œ ëª©ë¡
WINLOG_IP_CANDIDATES = [
    "winlog.event_data.SourceIp",
    "winlog.event_data.IpAddress",
    "source.ip",
    "destination.ip",
    "host.ip"
]
# ìœ íš¨í•˜ì§€ ì•Šì€ IP ì£¼ì†Œë¡œ ê°„ì£¼í•  ê°’ë“¤ì˜ ì§‘í•©
INVALID_IPS = {"-", "::1", "127.0.0.1"}

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜(Utility Functions) ---

def _get_nested_value(data: Dict[str, Any], path: str) -> Any:
    """
    ì (.)ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ë¡œë¥¼ ë”°ë¼ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°’ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    ê²½ë¡œì˜ ì¤‘ê°„ì— í‚¤ê°€ ì—†ê±°ë‚˜ í•´ë‹¹ ê°’ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    :param data: íƒìƒ‰í•  ë”•ì…”ë„ˆë¦¬
    :param path: ì ìœ¼ë¡œ êµ¬ë¶„ëœ í‚¤ ê²½ë¡œ (e.g., "winlog.event_data.SourceIp")
    :return: ì°¾ì€ ê°’ ë˜ëŠ” None
    """
    keys = path.split('.')
    val = data
    for k in keys:
        if not isinstance(val, dict):
            return None
        val = val.get(k)
        if val is None:
            return None
    return val

def get_ip_from_log(log_data: Dict[str, Any], candidates: List[str]) -> Optional[str]:
    """
    ì£¼ì–´ì§„ ë¡œê·¸ ë°ì´í„°ì™€ í›„ë³´ ê²½ë¡œ ëª©ë¡ì—ì„œ ìœ íš¨í•œ IP ì£¼ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    ì²« ë²ˆì§¸ë¡œ ë°œê²¬ë˜ëŠ” ìœ íš¨í•œ IP ì£¼ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    :param log_data: IP ì£¼ì†Œë¥¼ ì°¾ì„ ë¡œê·¸ ë°ì´í„°(ë”•ì…”ë„ˆë¦¬)
    :param candidates: IP ì£¼ì†Œ í›„ë³´ í•„ë“œ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    :return: ì°¾ì€ IP ì£¼ì†Œ ë¬¸ìì—´ ë˜ëŠ” None
    """
    for path in candidates:
        ip = _get_nested_value(log_data, path)
        if isinstance(ip, str) and ip not in INVALID_IPS:
            return ip
    return None

# --- ì„œë¹„ìŠ¤ í´ë˜ìŠ¤(Service Class) ---

class AnalysisService:
    """
    ìˆ˜ì§‘ëœ ë¡œê·¸ì™€ íŠ¸ë˜í”½ì„ íš¨ìœ¨ì ì¸ ì¼ê´„ ì²˜ë¦¬(batch) ë°©ì‹ìœ¼ë¡œ ë¶„ì„í•˜ê³  ëŒ€ì‘í•©ë‹ˆë‹¤.
    - Kafkaë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - Elasticsearchì— ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìœ„í˜‘ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - íƒì§€ëœ ê³µê²© ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³  ëŒ€ì‘ ì¡°ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    async def process_winlogbeat_logs_batch(self, messages: List[dict]):
        """Kafkaì—ì„œ ë°›ì€ Winlogbeat ë¡œê·¸ ë©”ì‹œì§€ë“¤ì„ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not messages: return

        es_actions, logs_to_process = [], []
        # 1. ë©”ì‹œì§€ ìˆœíšŒ: ES ì €ì¥ ì‘ì—… ëª©ë¡ ìƒì„± ë° ì˜ˆì¸¡í•  ë¡œê·¸ ë¶„ë¦¬
        for data in messages:
            log_data = data.get("log_data", {})
            if not log_data: continue
            
            log_id = str(uuid.uuid4()) # ê° ë¡œê·¸ì— ê³ ìœ  ID ë¶€ì—¬
            # Elasticsearchì— ì €ì¥í•  ë¬¸ì„œ(document) ìƒì„±
            es_doc = {
                "@timestamp": log_data.get("@timestamp", datetime.now(timezone.utc).isoformat()),
                "agent_id": data.get("agent_id", "unknown"),
                "hostname": data.get("host", {}).get("name"),
                "log_source": "winlogbeat",
                **log_data
            }
            es_actions.append({"_index": settings.es_index_winlogbeat, "_id": log_id, "_source": es_doc})
            logs_to_process.append({"log_id": log_id, "log_data": log_data})

        # 2. Elasticsearchì— ì¼ê´„ ì €ì¥ (Bulk Insert)
        if es_actions:
            try:
                await async_bulk(es_client, es_actions)
                logger.info(f"âœ… Winlogbeat ë¡œê·¸ {len(es_actions)}ê±´ ES ì €ì¥ ì„±ê³µ.")
            except Exception as e:
                logger.error(f"âŒ Winlogbeat ë¡œê·¸ ES ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
                return # ES ì €ì¥ ì‹¤íŒ¨ ì‹œ í›„ì† ì²˜ë¦¬ ì¤‘ë‹¨

        # 3. ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥
        async with AsyncSessionLocal() as db_session:
            processed_features_for_prediction, log_info_map = [], []
            
            # 3-1. ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬
            for log_info in logs_to_process:
                # Sysmon ë¡œê·¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ìœ¼ë¡œ ë§¤í•‘
                mapped_features = map_sysmon_to_model_columns(log_info["log_data"])
                # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° ë° ë§ˆìŠ¤í‚¹
                processed_features = fill_and_mask_missing_features(mapped_features)
                processed_features_for_prediction.append(processed_features)
                log_info_map.append(log_info) # ì˜ˆì¸¡ ê²°ê³¼ì™€ ë§¤ì¹­í•˜ê¸° ìœ„í•´ ì›ë³¸ ì •ë³´ ì €ì¥

            if not processed_features_for_prediction: return

            # 3-2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¼ê´„ ì˜ˆì¸¡ ì‹¤í–‰
            logger.info(f"ğŸ”® ì´ {len(processed_features_for_prediction)}ê±´ì˜ ë¡œê·¸ì— ëŒ€í•´ ì¼ê´„ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            prediction_start_time = time.perf_counter()
            # ë™ê¸° í•¨ìˆ˜ì¸ predictorë¥¼ ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë£¨í”„ì—ì„œ ì°¨ë‹¨ ì—†ì´ ì‹¤í–‰
            predictions = await asyncio.to_thread(predictor.predict_log_threat_batch, processed_features_for_prediction)
            prediction_end_time = time.perf_counter()
            logger.info(f"â±ï¸ Winlogbeat ì¼ê´„ ì˜ˆì¸¡ ì‹œê°„: {prediction_end_time - prediction_start_time:.4f} ì´ˆ")

            attack_logs_to_save = []
            # 3-3. ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
            for original_info, (label, score) in zip(log_info_map, predictions):
                try:
                    # ê³µê²© ì¡°ê±´: ë ˆì´ë¸”ì´ 'ì •ìƒ'ì´ ì•„ë‹ˆê³ , ì‹ ë¢°ë„ ì ìˆ˜ê°€ ì„ê³„ê°’(0.8) ì´ìƒ
                    is_attack = (label != "ì •ìƒ") and (label != "Prediction Error") and (score >= 0.8)
                    if is_attack:
                        log_data, log_id = original_info["log_data"], original_info["log_id"]
                        logger.warning(f"âš ï¸ ê³µê²© íƒì§€ë¨ [Winlogbeat]: Type={label}, Score={score:.4f}")
                        
                        # Redisì— ìœ„í˜‘ í†µê³„ ì—…ë°ì´íŠ¸ ë° ëŒ€ì‘ ì¡°ì¹˜ ë°œí–‰
                        await redis_client.hincrby("threat_stats", label, 1)
                        source_ip = get_ip_from_log(log_data, WINLOG_IP_CANDIDATES)
                        dest_port_str = _get_nested_value(log_data, "winlog.event_data.DestinationPort")

                        if source_ip:
                            await redis_client.publish(settings.redis_attack_channel, json.dumps({"action": "block_ip", "ip": source_ip}))
                            logger.info(f"ğŸš€ IP ì°¨ë‹¨ ëª…ë ¹ ìƒì„±: ip={source_ip}")
                        if dest_port_str:
                            try:
                                dest_port = int(dest_port_str)
                                await redis_client.publish(settings.redis_attack_channel, json.dumps({"action": "block_port", "port": dest_port}))
                                logger.info(f"ğŸš€ í¬íŠ¸ ì°¨ë‹¨ ëª…ë ¹ ìƒì„±: port={dest_port}")
                            except (ValueError, TypeError): pass
                        
                        # DBì— ì €ì¥í•  AttackLog ê°ì²´ ìƒì„±
                        attack_log_id = int(hashlib.sha1(log_id.encode()).hexdigest(), 16) % (10**12)
                        details = {
                            "rule_name": _get_nested_value(log_data, "winlog.event_data.RuleName"),
                            "process_guid": _get_nested_value(log_data, "winlog.event_data.ProcessGuid"),
                            "process_path": _get_nested_value(log_data, "winlog.event_data.Image"),
                            "user": log_data.get("user", {}).get("name"),
                            "es_log_id": log_id,
                            "es_log_index": settings.es_index_winlogbeat
                        }
                        new_attack = AttackLog(
                            log_attack_id=attack_log_id,
                            detected_at=datetime.now(timezone.utc),
                            attack_type=label, severity="High",
                            confidence=round(score * 100, 2),
                            source_address=source_ip,
                            hostname=log_data.get("host", {}).get("name"),
                            user_id=log_data.get('user_id'),
                            description=details,
                            response_type="Auto-detected",
                            responded_at=datetime.now(timezone.utc),
                            notification=False
                        )
                        attack_logs_to_save.append(new_attack)
                    else:
                        logger.info(f"âœ… ì •ìƒ ë¡œê·¸ [Winlogbeat]: Type={label}")
                except Exception as e:
                    logger.error(f"âŒ Winlog ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # 3-4. íƒì§€ëœ ê³µê²© ë¡œê·¸ë“¤ì„ DBì— ì¼ê´„ ì €ì¥
            if attack_logs_to_save:
                db_save_start_time = time.perf_counter()
                try:
                    db_session.add_all(attack_logs_to_save)
                    await db_session.commit()
                    logger.info(f"âœ… ê³µê²© ë¡œê·¸ {len(attack_logs_to_save)}ê±´ DB ì €ì¥ ì„±ê³µ")
                except Exception as e:
                    await db_session.rollback() # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±
                    logger.error(f"âŒ ê³µê²© ë¡œê·¸ DB ì¼ê´„ ì €ì¥ ì‹¤íŒ¨: {e}")
                db_save_end_time = time.perf_counter()
                logger.info(f"â±ï¸ Winlogbeat DB ì €ì¥ ì‹œê°„: {db_save_end_time - db_save_start_time:.4f} ì´ˆ")

    # --- Packetbeat ì²˜ë¦¬ ê´€ë ¨ í—¬í¼(Helper) ë©”ì„œë“œ ---

    def _sanitize_raw_packetbeat_data(self, raw_doc: dict) -> dict:
        """Packetbeat ì›ë³¸ ë°ì´í„°ì— í•„ìˆ˜ í‚¤ê°€ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ ê¸°ë³¸ê°’ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        raw_doc.setdefault("destination", {}).setdefault("port", 0)
        raw_doc["destination"].setdefault("packets", 0)
        raw_doc["destination"].setdefault("bytes", 0)
        raw_doc.setdefault("source", {}).setdefault("packets", 0)
        raw_doc["source"].setdefault("bytes", 0)
        raw_doc.setdefault("network", {}).setdefault("protocol", "tcp")
        raw_doc["network"].setdefault("duration", 0)
        return raw_doc

    def _extract_traffic_fields(self, raw_doc: dict) -> dict:
        """Packetbeat ì›ë³¸ ë°ì´í„°ì—ì„œ ëª¨ë¸ ì˜ˆì¸¡ì— í•„ìš”í•œ í•„ë“œë“¤ì„ ì¶”ì¶œí•˜ê³  ì´ë¦„ì„ ë§¤í•‘í•©ë‹ˆë‹¤."""
        mapping = {"Dst_Port": "destination.port", "Protocol": "network.protocol", "Flow_Duration": "network.duration", "Tot_Fwd_Pkts": "source.packets", "Tot_Bwd_Pkts": "destination.packets", "TotLen_Fwd_Pkts": "source.bytes", "TotLen_Bwd_Pkts": "destination.bytes"}
        protocol_map = {'tcp': 6, 'udp': 17, 'icmp': 1} # í”„ë¡œí† ì½œ ì´ë¦„ì„ ìˆ«ìë¡œ ë³€í™˜
        extracted = {}
        for target, path in mapping.items():
            val = _get_nested_value(raw_doc, path)
            if target == "Protocol" and isinstance(val, str):
                extracted[target] = protocol_map.get(val.lower(), 0)
            else:
                extracted[target] = val or 0
        return extracted

    def _calculate_traffic_features(self, raw_data: RawTrafficData) -> pd.DataFrame:
        """ì¶”ì¶œëœ íŠ¸ë˜í”½ ë°ì´í„°ë¡œë¶€í„° íŒŒìƒ í”¼ì²˜(Feature)ë“¤ì„ ê³„ì‚°í•˜ì—¬ DataFrameì„ ìƒì„±í•©ë‹ˆë‹¤."""
        epsilon = 1e-9 # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ë¥¼ ìœ„í•œ ì‘ì€ ê°’
        duration_sec = (raw_data.Flow_Duration / 1_000_000) + epsilon # durationì€ ë³´í†µ ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„
        total_bytes = raw_data.TotLen_Fwd_Pkts + raw_data.TotLen_Bwd_Pkts
        total_pkts = raw_data.Tot_Fwd_Pkts + raw_data.Tot_Bwd_Pkts + epsilon
        features = {
            "Dst_Port": raw_data.Dst_Port, "Protocol": raw_data.Protocol, "Flow_Duration": raw_data.Flow_Duration,
            "Tot_Fwd_Pkts": raw_data.Tot_Fwd_Pkts, "Tot_Bwd_Pkts": raw_data.Tot_Bwd_Pkts,
            "TotLen_Fwd_Pkts": raw_data.TotLen_Fwd_Pkts, "TotLen_Bwd_Pkts": raw_data.TotLen_Bwd_Pkts,
            "Flow_Byts_per_s": total_bytes / duration_sec, "Flow_Pkts_per_s": total_pkts / duration_sec,
            "Fwd_Pkts_per_s": raw_data.Tot_Fwd_Pkts / duration_sec, "Bwd_Pkts_per_s": raw_data.Tot_Bwd_Pkts / duration_sec,
            "Down_per_Up_Ratio": raw_data.Tot_Bwd_Pkts / (raw_data.Tot_Fwd_Pkts + epsilon),
            "Pkt_Size_Avg": total_bytes / total_pkts,
            "Fwd_Seg_Size_Avg": raw_data.TotLen_Fwd_Pkts / (raw_data.Tot_Fwd_Pkts + epsilon),
            "Bwd_Seg_Size_Avg": raw_data.TotLen_Bwd_Pkts / (raw_data.Tot_Bwd_Pkts + epsilon)
        }
        # ëª¨ë¸ì´ í•™ìŠµëœ í”¼ì²˜ ìˆœì„œì™€ ë™ì¼í•˜ê²Œ DataFrame ì»¬ëŸ¼ ìˆœì„œë¥¼ ê³ ì •
        order = ["Dst_Port", "Protocol", "Flow_Duration", "Tot_Fwd_Pkts", "Tot_Bwd_Pkts", "TotLen_Fwd_Pkts", "TotLen_Bwd_Pkts", "Flow_Byts_per_s", "Flow_Pkts_per_s", "Fwd_Pkts_per_s", "Bwd_Pkts_per_s", "Down_per_Up_Ratio", "Pkt_Size_Avg", "Fwd_Seg_Size_Avg", "Bwd_Seg_Size_Avg"]
        return pd.DataFrame([features])[order]

    async def process_packetbeat_traffic_batch(self, messages: List[dict]):
        """Kafkaì—ì„œ ë°›ì€ Packetbeat íŠ¸ë˜í”½ ë©”ì‹œì§€ë“¤ì„ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ì´ ë©”ì„œë“œì˜ êµ¬ì¡°ëŠ” `process_winlogbeat_logs_batch`ì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤.
        if not messages: return

        es_actions, traffic_to_process = [], []
        # 1. ES ì €ì¥ ëª©ë¡ ìƒì„± ë° ì²˜ë¦¬í•  íŠ¸ë˜í”½ ë¶„ë¦¬
        for data in messages:
            raw_doc = data.get("traffic_data", {})
            if not raw_doc: continue
            log_id = str(uuid.uuid4())
            es_doc = {"@timestamp": raw_doc.get("@timestamp", datetime.now(timezone.utc).isoformat()), "agent_id": data.get("agent_id", "unknown"), "hostname": data.get("host", {}).get("name"), "log_source": "packetbeat", **raw_doc}
            es_actions.append({"_index": settings.es_index_packetbeat, "_id": log_id, "_source": es_doc})
            traffic_to_process.append({"log_id": log_id, "raw_traffic_doc": raw_doc})

        # 2. Elasticsearchì— ì¼ê´„ ì €ì¥
        if es_actions:
            try:
                await async_bulk(es_client, es_actions)
                logger.info(f"âœ… Packetbeat ë¡œê·¸ {len(es_actions)}ê±´ ES ì €ì¥ ì„±ê³µ.")
            except Exception as e:
                logger.error(f"âŒ Packetbeat ë¡œê·¸ ES ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
                return

        # 3. ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥
        async with AsyncSessionLocal() as db_session:
            features_df_list, traffic_info_map = [], []
            # 3-1. ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬
            for traffic_info in traffic_to_process:
                try:
                    cleaned_doc = self._sanitize_raw_packetbeat_data(traffic_info["raw_traffic_doc"])
                    extracted_fields = self._extract_traffic_fields(cleaned_doc)
                    raw_data = RawTrafficData.model_validate(extracted_fields) # Pydantic ëª¨ë¸ë¡œ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
                    final_features_df = self._calculate_traffic_features(raw_data)
                    features_df_list.append(final_features_df)
                    traffic_info_map.append({'log_id': traffic_info['log_id'], 'cleaned_doc': cleaned_doc, 'features_dict': final_features_df.to_dict('records')[0]})
                except (ValidationError, Exception) as e:
                    logger.error(f"âŒ Packetbeat ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            if not features_df_list: return

            # 3-2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¼ê´„ ì˜ˆì¸¡ ì‹¤í–‰
            batch_df = pd.concat(features_df_list, ignore_index=True) # ê°œë³„ DataFrameë“¤ì„ í•˜ë‚˜ë¡œ í•©ì³ ë°°ì¹˜ ì²˜ë¦¬
            
            logger.info(f"ğŸ”® ì´ {len(batch_df)}ê±´ì˜ íŠ¸ë˜í”½ì— ëŒ€í•´ ì¼ê´„ ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            prediction_start_time = time.perf_counter()
            predictions = await asyncio.to_thread(predictor.predict_traffic_threat_batch, batch_df)
            prediction_end_time = time.perf_counter()
            logger.info(f"â±ï¸ Packetbeat ì¼ê´„ ì˜ˆì¸¡ ì‹œê°„: {prediction_end_time - prediction_start_time:.4f} ì´ˆ")
            
            attack_traffics_to_save = []
            # 3-3. ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
            for item_info, label in zip(traffic_info_map, predictions):
                try:
                    is_attack = (label != "Benign") and (label != "Prediction Error")
                    if is_attack:
                        logger.warning(f"âš ï¸ ê³µê²© íƒì§€ë¨ [Packetbeat]: Type={label}")
                        
                        # Redis í†µê³„ ì—…ë°ì´íŠ¸ ë° ëŒ€ì‘ ì¡°ì¹˜ ë°œí–‰
                        await redis_client.hincrby("threat_stats", label, 1)
                        cleaned_doc = item_info['cleaned_doc']
                        source_ip = cleaned_doc.get("source", {}).get("ip")
                        dest_port = cleaned_doc.get("destination", {}).get("port")

                        if source_ip:
                            await redis_client.publish(settings.redis_attack_channel, json.dumps({"action": "block_ip", "ip": source_ip}))
                            logger.info(f"ğŸš€ IP ì°¨ë‹¨ ëª…ë ¹ ìƒì„±: ip={source_ip}")
                        if dest_port is not None:
                            await redis_client.publish(settings.redis_attack_channel, json.dumps({"action": "block_port", "port": dest_port}))
                            logger.info(f"ğŸš€ í¬íŠ¸ ì°¨ë‹¨ ëª…ë ¹ ìƒì„±: port={dest_port}")
                        
                        # DBì— ì €ì¥í•  AttackTraffic ê°ì²´ ìƒì„±
                        traffic_attack_id = int(hashlib.sha1(item_info["log_id"].encode()).hexdigest(), 16) % (10**12)
                        features_dict = item_info['features_dict']
                        new_attack = AttackTraffic(
                            traffic_attack_id=traffic_attack_id,
                            timestamp=datetime.now(timezone.utc),
                            user_id=item_info['cleaned_doc'].get("user_id"),
                            src_ip=source_ip,
                            dst_port=features_dict.get("Dst_Port"),
                            protocol=features_dict.get("Protocol"),
                            flow_duration=features_dict.get("Flow_Duration"),
                            tot_fwd_pkts=features_dict.get("Tot_Fwd_Pkts"),
                            tot_bwd_pkts=features_dict.get("Tot_Bwd_Pkts"),
                            flow_byts_per_s=features_dict.get("Flow_Byts_per_s"),
                            flow_pkts_per_s=features_dict.get("Flow_Pkts_per_s"),
                            down_per_up_ratio=features_dict.get("Down_per_Up_Ratio"),
                            # í˜„ì¬ ê³„ì‚°ë˜ì§€ ì•ŠëŠ” í”¼ì²˜ë“¤ì€ DB ìŠ¤í‚¤ë§ˆì— ë§ì¶° ê¸°ë³¸ê°’(0)ìœ¼ë¡œ ì„¤ì •, DB ì»¬ëŸ¼ ìˆ˜ì • í•„ìš”.
                            bwd_iat_tot=0,
                            fin_flag_cnt=0,
                            rst_flag_cnt=0,
                            psh_flag_cnt=0,
                            ack_flag_cnt=0,
                            urg_flag_cnt=0,
                            notification=False
                        )
                        attack_traffics_to_save.append(new_attack)
                    else:
                        logger.info(f"âœ… ì •ìƒ íŠ¸ë˜í”½ [Packetbeat]: Type={label}")
                except Exception as e:
                    logger.error(f"âŒ Packetbeat ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # 3-4. íƒì§€ëœ ê³µê²© íŠ¸ë˜í”½ë“¤ì„ DBì— ì¼ê´„ ì €ì¥
            if attack_traffics_to_save:
                db_save_start_time = time.perf_counter()
                try:
                    db_session.add_all(attack_traffics_to_save)
                    await db_session.commit()
                    logger.info(f"âœ… ê³µê²© íŠ¸ë˜í”½ {len(attack_traffics_to_save)}ê±´ DB ì €ì¥ ì„±ê³µ")
                except Exception as e:
                    await db_session.rollback()
                    logger.error(f"âŒ ê³µê²© íŠ¸ë˜í”½ DB ì¼ê´„ ì €ì¥ ì‹¤íŒ¨: {e}")
                db_save_end_time = time.perf_counter()
                logger.info(f"â±ï¸ Packetbeat DB ì €ì¥ ì‹œê°„: {db_save_end_time - db_save_start_time:.4f} ì´ˆ")

    async def get_threat_statistics(self) -> dict:
        """Redisì—ì„œ ìœ„í˜‘ í†µê³„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # Redis í•´ì‹œ(hash)ì—ì„œ ëª¨ë“  í•„ë“œì™€ ê°’ì„ ê°€ì ¸ì˜´
            stats = await redis_client.hgetall("threat_stats")
            # Redisì—ì„œ ë°›ì€ ë°ì´í„°ëŠ” byte-stringì´ë¯€ë¡œ, keyëŠ” utf-8ë¡œ ë””ì½”ë”©í•˜ê³  valueëŠ” ì •ìˆ˜ë¡œ ë³€í™˜
            return {key.decode('utf-8'): int(value) for key, value in stats.items()}
        except Exception as e:
            logger.error(f"âŒ Redisì—ì„œ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}

# AnalysisService í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ ì‚¬ìš©
analysis_service = AnalysisService()